{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "black-health",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchsummary import summary\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from networks import SegNet\n",
    "# from load_nyuv2_dataset import load_dataset\n",
    "\n",
    "# Constant variables\n",
    "EPOCHS = 100\n",
    "DEVICE =  torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 20\n",
    "LR = 0.0001  #0.000\n",
    "INPUT_CHANNELS = 3\n",
    "OUTPUT_CHANNELS = 14\n",
    "IMG_SIZE = (288, 384)\n",
    "# Paths\n",
    "\n",
    "\n",
    "CHECKPOINT_DIR = '/home4/shubham/MTML_Pth/checkpoints/'\n",
    "VIS_RESULTS_PATH = '/home4/shubham/MTML_Pth/results/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "authorized-diary",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetLoader(Dataset):\n",
    "    \n",
    "    def __init__(self, data, ground_truth, transform = None):\n",
    "        self.data = data\n",
    "        self.gt = ground_truth\n",
    "        self.length = len(data)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "  \n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.data[idx])\n",
    "        img = img.resize(IMG_SIZE, Image.BILINEAR)\n",
    "        gt = Image.open(self.gt[idx])\n",
    "        gt = gt.resize(IMG_SIZE, Image.NEAREST)\n",
    "        \n",
    "        if self.transform:\n",
    "            img, gt = self.transform(img, gt)\n",
    "            gt = np.array(gt)\n",
    "            \n",
    "        return img, gt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "convinced-arthritis",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    Early stops the training if validation loss doesn't improve after a given patience.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path=CHECKPOINT_DIR+'early_stopping_sgd_segmentation_model.pth'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 10\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'early_stopping_vgg16model.pth'   \n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "    \n",
    "    def __call__(self, val_loss, model):\n",
    "        \n",
    "        score = -val_loss\n",
    "        \n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            \n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            \n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "                \n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0   \n",
    "    \n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        \"\"\"\n",
    "        saves the current best version of the model if there is decrease in validation loss\n",
    "        \"\"\"\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.vall_loss_min = val_loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aggressive-quarter",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfMatrix(object):\n",
    "    def __init__(self, num_classes):\n",
    "        self.num_classes = num_classes\n",
    "        self.mat = None\n",
    "\n",
    "    def update(self, pred, target):\n",
    "        n = self.num_classes\n",
    "        if self.mat is None:\n",
    "            self.mat = torch.zeros((n, n), dtype=torch.int64, device=pred.device)\n",
    "        with torch.no_grad():\n",
    "            k = (target >= 0) & (target < n)\n",
    "            inds = n * target[k].to(torch.int64) + pred[k]\n",
    "            self.mat += torch.bincount(inds, minlength=n ** 2).reshape(n, n)\n",
    "\n",
    "    def get_metrics(self):\n",
    "        h = self.mat.float()\n",
    "        acc = torch.diag(h).sum() / h.sum()\n",
    "        iu = torch.diag(h) / (h.sum(1) + h.sum(0) - torch.diag(h))\n",
    "        return torch.mean(iu), acc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "appointed-withdrawal",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, tloader, vloader, criterion, optimizer):\n",
    "    \"\"\"\n",
    "    returns loss and accuracy of the model for 1 epoch.\n",
    "    params: model -  vgg16\n",
    "          tloader - train dataset\n",
    "          vloader - val dataset\n",
    "          criterion - loss function\n",
    "          optimizer - Adam optimizer\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    t_mean_iou = []\n",
    "    v_mean_iou = []\n",
    "    \n",
    "    model.train()\n",
    "    model.to(DEVICE)\n",
    "    train_cm = ConfMatrix(OUTPUT_CHANNELS)\n",
    "    test_cm = ConfMatrix(OUTPUT_CHANNELS)\n",
    "    \n",
    "    for ind, (image, label) in enumerate(tloader):\n",
    "     \n",
    "        image = image.to(DEVICE)\n",
    "        label = label.type(torch.LongTensor)\n",
    "        label = label.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output, _ = model(image)\n",
    "        \n",
    "        loss = criterion(output, torch.squeeze(label,1))\n",
    "        train_losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        pred_values, predicted = torch.max(output, 1)\n",
    "        train_cm.update(predicted, label)\n",
    "       \n",
    "    \n",
    "    t_epoch_iou, train_accuracy = train_cm.get_metrics()    \n",
    "    t_epoch_loss = np.average(train_losses)\n",
    "    \n",
    "\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for ind, (image, label) in enumerate(vloader):\n",
    "            image = image.to(DEVICE)\n",
    "            label = label.type(torch.LongTensor)\n",
    "            label = label.to(DEVICE)\n",
    "            output,_ = model(image)\n",
    "            loss = criterion(output, torch.squeeze(label,1))\n",
    "            pred_values, predicted = torch.max(output, 1)\n",
    "            test_cm.update(predicted, label)\n",
    "            valid_losses.append(loss.item())\n",
    "            \n",
    "            if ind == 0:\n",
    "                fig = plt.figure(figsize=(11,11))\n",
    "                ax = plt.subplot(1, 3, 1)\n",
    "                plt.imshow(image[0].cpu().numpy().transpose((1, 2, 0)))\n",
    "                ax = plt.subplot(1, 3, 2)\n",
    "                plt.imshow(torch.squeeze(label[0].cpu(),0))\n",
    "                ax = plt.subplot(1, 3, 3)\n",
    "                plt.imshow(predicted[0].cpu())\n",
    "                plt.show()\n",
    "                fig2 = plt.figure(figsize=(11,11))\n",
    "                ax = plt.subplot(1, 3, 1)\n",
    "                plt.imshow(image[1].cpu().numpy().transpose((1, 2, 0)))\n",
    "                ax = plt.subplot(1, 3, 2)\n",
    "                plt.imshow(torch.squeeze(label[1].cpu(),0))\n",
    "                ax = plt.subplot(1, 3, 3) \n",
    "                plt.imshow(predicted[1].cpu())\n",
    "                plt.show()\n",
    "    \n",
    "    v_epoch_loss = np.average(valid_losses)\n",
    "    v_epoch_iou, val_accuracy = test_cm.get_metrics() \n",
    "    \n",
    "    return model, t_epoch_loss, v_epoch_loss, train_accuracy, val_accuracy, t_epoch_iou, v_epoch_iou\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "parental-mason",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(trainloader, valloader):\n",
    "    \"\"\"\n",
    "    returns losses (train and val), accuracies (train and val), trained_model\n",
    "    params: trainloader = train dataset\n",
    "            valloader = validation dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    model = SegNet(INPUT_CHANNELS, OUTPUT_CHANNELS).to(DEVICE)\n",
    "    \n",
    "    criterion = torch.nn.CrossEntropyLoss().to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "    \n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    train_acc = []\n",
    "    val_acc = []\n",
    "    \n",
    "    early_stop = EarlyStopping(patience=7)\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        print(\"Running Epoch {}\".format(epoch+1))\n",
    "\n",
    "        model, epoch_train_loss,  epoch_val_loss, train_ac, val_ac, train_iou, val_iou = train_loop( model, trainloader, valloader, criterion, optimizer)\n",
    "        train_loss.append(epoch_train_loss)   \n",
    "        val_loss.append(epoch_val_loss)\n",
    "        train_acc.append(train_ac)\n",
    "        val_acc.append(val_ac)\n",
    "\n",
    "        print(\"Training loss: {0:.4f}   Training accuracy: {1:.4f}   Training mIoU: {2:.4f}\".format(epoch_train_loss, train_ac, train_iou))\n",
    "        print(\"Validation loss: {0:.4f} Validation accuracy: {1:.4f} Validation mIoU: {2:.4f}\".format(epoch_val_loss, val_ac, val_iou))\n",
    "        print(\"--------------------------------------------------------\")\n",
    "        \n",
    "        early_stop(epoch_val_loss, model)\n",
    "    \n",
    "        if early_stop.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break \n",
    "\n",
    "        if (epoch+1)%5 == 0:\n",
    "            torch.save(model.state_dict(), CHECKPOINT_DIR + \"/segnet_epoch_\" + str(epoch+1) + \".pth\")\n",
    "\n",
    "    print(\"Training completed!\")\n",
    "    losses = [train_loss, val_loss]\n",
    "    accuracies = [train_acc, val_acc]\n",
    "    \n",
    "    return losses, accuracies, model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "animal-oxide",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(testloader):\n",
    "    \"\"\"\n",
    "    returns performance of the model on test dataset\n",
    "    \"\"\"\n",
    "    correct, total = 0, 0\n",
    "    model = SegNet(INPUT_CHANNELS, OUTPUT_CHANNELS).to(DEVICE)\n",
    "    criterion = torch.nn.CrossEntropyLoss().to(DEVICE)\n",
    "    v_mean_iou = []\n",
    "    valid_losses = []\n",
    "    model.eval()\n",
    "    print(\"Loading pre-trained weights...\")\n",
    "    final_model_path = CHECKPOINT_DIR+'early_stopping_segmentation_model.pth'\n",
    "    model.load_state_dict(torch.load(final_model_path))\n",
    "    print(\"Weights loaded!\")\n",
    "    test_cm = ConfMatrix(OUTPUT_CHANNELS)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for ind, (image, label) in enumerate(testloader):\n",
    "            image = image.to(DEVICE)\n",
    "            label = label.type(torch.LongTensor)\n",
    "            label = label.to(DEVICE)\n",
    "            output,_ = model(image)\n",
    "            pred_values, predicted = torch.max(output, 1)\n",
    "            test_cm.update()\n",
    "            \n",
    "            for i in range(25):\n",
    "                fig = plt.figure(figsize=(11,11))\n",
    "                ax = plt.subplot(1, 3, 1)\n",
    "                plt.imshow(image[i].cpu().numpy().transpose((1, 2, 0)))\n",
    "                ax = plt.subplot(1, 3, 2)\n",
    "                plt.imshow(torch.squeeze(label[i].cpu(),0))\n",
    "                ax = plt.subplot(1, 3, 3)\n",
    "                plt.imshow(predicted[i].cpu())\n",
    "                plt.show()\n",
    "                if i == 5:\n",
    "                    break\n",
    "        \n",
    "        iou, accuracy = test_cm.get_metrics()\n",
    "        print(\"Mean Pixel IoU: \",iou)\n",
    "        print(\"Accuracy: \",accuracy)\n",
    "   \n",
    "    \n",
    "\n",
    "def train_transformation(image, gt):\n",
    "    \n",
    "    p = np.random.uniform(0, 1)\n",
    "    \n",
    "    if p <= 0.5 :\n",
    "        img_out = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        img = torchvision.transforms.ToTensor()(img_out)\n",
    "        gt = gt.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        \n",
    "    else:\n",
    "        img = torchvision.transforms.ToTensor()(image)\n",
    "    \n",
    "    return img, gt\n",
    "\n",
    "def test_transformation(image, gt):\n",
    "    img = torchvision.transforms.ToTensor()(image)\n",
    "    return img, gt\n",
    "\n",
    "\n",
    "def get_data_loader(data, label, flag):\n",
    "    \"\"\"\n",
    "    returns train/test/val dataloaders\n",
    "    params: flag = train/test/val\n",
    "    \"\"\"\n",
    "    if flag == \"train\":\n",
    "        dataset = DatasetLoader(data[flag], label[flag], transform=train_transformation) \n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size = BATCH_SIZE, shuffle=True, num_workers=4)\n",
    " \n",
    "    else:\n",
    "        dataset = DatasetLoader(data[flag], label[flag], transform=test_transformation)\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size = BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "def draw_training_curves(train_losses, test_losses,curve_name):\n",
    "    plt.clf()\n",
    "   \n",
    "    plt.plot(train_losses, label='Training {}'.format(curve_name))\n",
    "    plt.plot(test_losses, label='Testing {}'.format(curve_name))\n",
    "    plt.legend(frameon=False)\n",
    "    plt.savefig(VIS_RESULTS_PATH + \"/{}_sgd_segmentation.png\".format(curve_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adequate-mathematics",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data, labels = load_dataset(\"segmentation\")\n",
    "train_loader = get_data_loader(data, labels, \"train\")\n",
    "val_loader = get_data_loader(data, labels,\"val\")\n",
    "test_loader = get_data_loader(data, labels,\"test\")\n",
    "\n",
    "# train model\n",
    "losses, accuracies, model = train_model(train_loader, val_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "soviet-platform",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# plot trained metrics\n",
    "loss_curve = \"loss\"\n",
    "draw_training_curves(losses[0], losses[1],loss_curve)\n",
    "loss_curve = \"accuracy\"\n",
    "draw_training_curves(accuracies[0], accuracies[1],loss_curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "injured-alpha",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = get_data_loader(data, labels,\"test\")\n",
    "run_inference(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thousand-humanitarian",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
